<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>End-to-end RL Improves Dexterous Grasping Policies</title>
	<meta name="description" content="Scaling End-to-end Vision-based RL">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="assets/css/style.css">
	<link rel="icon" href="assets/img/favicon.svg" type="image/svg+xml">
</head>
<body>
	<header class="site-header">
		<div class="container">
			<h1 class="site-title">End-to-end RL Improves Dexterous Grasping Policies</h1>
			<p class="site-subtitle">Scaling End-to-end Vision-based RL</p>
			<div class="authors">
				<div class="author-list">
					<a href="https://www.ritvik-singh.com/" target="_blank" rel="noopener">Ritvik Singh<sup>1,2</sup></a>,
					<a href="https://scholar.google.com/citations?user=TCYAoF8AAAAJ&hl=en" target="_blank" rel="noopener">Karl Van Wyk<sup>1</sup></a>,
					<a href="https://people.eecs.berkeley.edu/~malik/" target="_blank" rel="noopener">Jitendra Malik<sup>2</sup></a>,
					<a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank" rel="noopener">Pieter Abbeel<sup>2</sup></a>,
					<a href="https://www.nathanratliff.com/" target="_blank" rel="noopener">Nathan Ratliff<sup>1</sup></a>,
					<a href="https://ankurhanda.github.io/" target="_blank" rel="noopener">Ankur Handa<sup>1</sup></a>
				</div>
				<div class="affiliations">
					<sup>1</sup>NVIDIA<br>
					<sup>2</sup>University of California, Berkeley<br>
				</div>
			</div>
			<div class="links">
				<a class="btn" href="assets/pdf/e2e_grasping.pdf" target="_blank" rel="noopener">Paper</a>
				<a class="btn" href="#videos">Videos</a>
				<a class="btn" href="#method">Method</a>
				<a class="btn" href="#results">Results</a>
			</div>
			<div class="hero-video">
				<video src="assets/videos/e2e_title_4k_hw.mp4" playsinline muted autoplay loop preload="metadata" aria-label="Project overview video"></video>
				<div class="hero-controls" role="group" aria-label="Video controls">
					<button class="hero-btn hero-play" aria-label="Pause video" title="Pause"><span class="icon icon-pause" aria-hidden="true"></span></button>
					<div class="hero-progress">
						<input class="hero-seek" type="range" min="0" max="100" value="0" step="0.1" aria-label="Seek">
					</div>
					<div class="hero-time"><span class="tcur">0:00</span> / <span class="ttotal">0:00</span></div>
				</div>
			</div>
		</div>
	</header>

	<main>
		<section id="abstract" class="section">
			<div class="container narrow">
				<h2>Abstract</h2>
				<p>
					This work explores techniques to scale up image-based end-to-end learning for dexterous grasping with an arm + hand system. Unlike state-based RL, vision-based RL is much more memory inefficient, resulting in relatively low batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is still an attractive method as unlike the more commonly used techniques which distill state-based policies into vision networks, end-to-end RL can allow for emergent active vision behaviors. We identify a key bottleneck in training these policies is the way most existing simulators scale to multiple GPUs using traditional data parallelism techniques. We propose a new method where we disaggregate the simulator and RL (both training and experience buffers) onto separate GPUs. On a node with four GPUs, we have the simulator running on three of them, and PPO running on the fourth. We are able to show that with the same number of GPUs, we can double the number of existing environments compared to the previous baseline of standard data parallelism. This allows us to train vision-based environments, end-to-end with depth, which were previously performing far worse with the baseline. We train and distill both depth and state-based policies into stereo RGB networks and show that depth distillation leads to better results, both in simulation and reality. This improvement is likely due to the observability gap between state and vision policies which does not exist when distilling depth policies into stereo RGB. We further show that the increased batch size brought about by disaggregated simulation also improves real world performance. When deploying in the real world, we improve upon the previous state-of-the-art vision-based results using our end-to-end policies. To our knowledge, this is the first work that has demonstrated end-to-end RL for dexterous grasping with multifingered hands. 
				</p>
				<h3 class="subheading">Contributions</h3>
				<ul class="contribs">
					<li>First sim-to-real with multifingered hands using end-to-end RL training.</li>
					<li>Improved simulation infrastructure to scale up vision-based training.</li>
					<li>State of the art results for vision-based grasping.</li>
				</ul>
			</div>
		</section>

		<section id="videos" class="section section-alt">
			<div class="container">
				<h2>Videos</h2>
				<div class="gallery">
					<div class="card video-card" aria-label="Video 1">
						<video src="assets/videos/video_1.mp4" playsinline muted autoplay loop preload="metadata"></video>
						<div class="v-controls" role="group" aria-label="Inline video controls">
							<button class="v-btn v-play" aria-label="Pause video" title="Pause"><span class="icon icon-pause" aria-hidden="true"></span></button>
							<div class="v-progress"><input class="v-seek" type="range" min="0" max="100" value="0" step="0.1" aria-label="Seek"></div>
							<div class="v-time"><span class="v-cur">0:00</span> / <span class="v-total">0:00</span></div>
						</div>
					</div>
					<div class="card video-card" aria-label="Video 2">
						<video src="assets/videos/video_2.mp4" playsinline muted autoplay loop preload="metadata"></video>
						<div class="v-controls" role="group" aria-label="Inline video controls">
							<button class="v-btn v-play" aria-label="Pause video" title="Pause"><span class="icon icon-pause" aria-hidden="true"></span></button>
							<div class="v-progress"><input class="v-seek" type="range" min="0" max="100" value="0" step="0.1" aria-label="Seek"></div>
							<div class="v-time"><span class="v-cur">0:00</span> / <span class="v-total">0:00</span></div>
						</div>
					</div>
					<div class="card video-card" aria-label="Video 3">
						<video src="assets/videos/video_3.mp4" playsinline muted autoplay loop preload="metadata"></video>
						<div class="v-controls" role="group" aria-label="Inline video controls">
							<button class="v-btn v-play" aria-label="Pause video" title="Pause"><span class="icon icon-pause" aria-hidden="true"></span></button>
							<div class="v-progress"><input class="v-seek" type="range" min="0" max="100" value="0" step="0.1" aria-label="Seek"></div>
							<div class="v-time"><span class="v-cur">0:00</span> / <span class="v-total">0:00</span></div>
						</div>
					</div>
				</div>
			</div>
		</section>

		<section id="method" class="section">
			<div class="container">
				<h2>Method</h2>
				<div class="method-subsection">
					<h3>End-to-end RL</h3>
					<p class="muted">Vision-based policies are incredibly important for real-world manipulation. However, training them directly from RL has historically been challenging due to the high sample complexity of image space. This has led to two-stage methods gaining prominence whereby a state-based teacher policy is trained with RL and then distilled into a vision-based student policy. While this has led to training successful policies, they fundamentally do not learn vision-aware behaviors. For example, imagine a robot arm trying to grasp an object. Suppose the arm is currently occluding said object. The teacher policy, which has access to the groundtruth object position, will simply just pick it up. However, the student policy may struggle to recreate this behavior as it has not learned to move the arm out of the way in order to see the object. In such cases, the student is trying to mimic state-based behaviors while only having access to vision information which causes it to act sub-optimally with respect to its inputs. Therefore, end-to-end training, where the RL policy learns directly from images, will lead to policy behaviors that lend themselves better to their sensory modalities.
                        However, RGB end-to-end RL is much slower than depth-based RL as the rendering process for accurate light transport simulation is far more time consuming. Thus, a suitable middle ground that meets the requirements above while also being able to run on a reasonable hardware budget is to train a depth based policy with RL, and distill this into a stereo RGB-based policy in order to deploy in the real world.</p>
				</div>
				<div class="method-subsection">
					<h3>Disaggregated Simulation vs Data Parallelism</h3>
					<p class="muted">When training vision-based RL, it's important to have a sufficiently large batch size to get a reliable learning signal from PPO. The standard method of scaling up is data parallelism. Our new method, disaggregated simulation and RL, is able to double the number of environments with the same hardware. The diagrams below illustrate the data flow across 4 GPUs for both data parallelism and disaggregated simulation with horizon length = 3. Click the buttons to view the animation.</p>
					<div class="method-controls">
						<button class="btn" data-mode="data-parallel">Data Parallelism</button>
						<button class="btn" data-mode="disaggregated">Disaggregated Simulation</button>
					</div>
					<div class="method-diagrams">
						<svg class="method-svg" id="svg-data-parallel" viewBox="0 0 1000 300" aria-label="Data Parallelism diagram"></svg>
						<svg class="method-svg" id="svg-disaggregated" viewBox="0 0 1000 300" aria-label="Disaggregated Simulation diagram"></svg>
					</div>
				</div>
			</div>
		</section>

		<section id="results" class="section section-alt">
			<div class="container">
				<h2>Results</h2>
				<p class="muted">We evaluate scaling capacity and policy performance in both simulation and the real world.</p>

				<div class="results-grid">
					<article class="result-card">
						<header class="result-head">
							<h3>Scaling capacity on a 4×GPU node</h3>
							<p class="muted">Maximum concurrent environments at different input resolutions.</p>
						</header>
						<div class="result-body">
							<table class="table">
								<thead>
									<tr>
										<th>Input resolution</th>
										<th>Data Parallel</th>
										<th>Disaggregated Simulation</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td>160×120</td>
										<td><div class="subcell">1024 / GPU</div><div class="subcell muted">(4096 total)</div></td>
										<td class="cell-best"><div class="subcell">2800 / GPU</div><div class="subcell muted">(8400 total)</div></td>
									</tr>
									<tr>
										<td>320×240</td>
										<td><div class="subcell">256 / GPU</div><div class="subcell muted">(1024 total)</div></td>
										<td class="cell-best"><div class="subcell">700 / GPU</div><div class="subcell muted">(2100 total)</div></td>
									</tr>
								</tbody>
							</table>
							<p class="footnote muted">Data Parallel: 4× sim+RL GPUs.</p>
                            <p class="footnote muted">Disaggregated Simulation: 3×simulators + 1×learner GPUs.</p>
						</div>
					</article>

					<article class="result-card">
						<header class="result-head">
							<h3>Simulation progress and success</h3>
							<p class="muted">Data Parallel (DP) vs Disaggregated Simulation (Disagg)at 160×120 and 320×240 (5 seeds avg) for e2e depth RL.</p>
						</header>
						<div class="result-body">
							<table class="table">
								<thead>
									<tr>
										<th>Res.</th>
										<th>Method</th>
										<th>ADR Inc. ↑</th>
										<th>% Full ADR ↑</th>
										<th>SR ↑</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td rowspan="2">160×120</td>
										<td>DP</td>
										<td>0.38</td>
										<td>20%</td>
										<td>0.37</td>
									</tr>
									<tr>
										<td class="method-disagg">Disagg</td>
										<td class="cell-best">1.00</td>
										<td class="cell-best">100%</td>
										<td class="cell-best">0.42</td>
									</tr>
									<tr>
										<td rowspan="2">320×240</td>
										<td>DP</td>
										<td>0.00</td>
										<td>0%</td>
										<td>0.00</td>
									</tr>
									<tr>
										<td class="method-disagg">Disagg</td>
										<td class="cell-best">0.90</td>
										<td class="cell-best">20%</td>
										<td class="cell-best">0.35</td>
									</tr>
								</tbody>
							</table>
						</div>
					</article>

					<!-- Inserted plot card as third item (bottom-left) -->
					<article class="result-card">
						<header class="result-head">
							<h3>Depth vs State Teachers Distillation</h3>
							<p class="muted">Simulation performance between distilled stereo RGB policies.</p>
						</header>
						<div class="result-body">
							<img id="chart-sim-depth-vs-state"
								class="result-chart"
								alt="Depth vs State teachers chart"
								data-src="assets/img/sim_depth_vs_state_new.png" />
						</div>
					</article>

					<article class="result-card">
						<header class="result-head">
							<h3>Real-world success rates</h3>
							<p class="muted">Depth teachers lead to better results; disagg further improves performance.</p>
						</header>
						<div class="result-body">
							<table class="table">
								<thead>
									<tr>
										<th>Model</th>
										<th>Success Rate ↑</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td>DextrAH-G (state teacher)</td>
										<td>87%</td>
									</tr>
									<tr>
										<td>DextrAH-RGB (state teacher)</td>
										<td>77%</td>
									</tr>
									<tr>
										<td>Ours (depth teacher)</td>
										<td>87%</td>
									</tr>
									<tr>
										<td>Ours (depth teacher, disagg)</td>
										<td class="cell-best">93%</td>
									</tr>
								</tbody>
							</table>
						</div>
					</article>
				</div>

				<!-- Plot card moved above into the grid; standalone removed -->
			</div>
		</section>

		
	</main>

	<footer class="site-footer">
		<div class="container">
			
		</div>
	</footer>

	<div class="lightbox" id="lightbox" aria-hidden="true" role="dialog" aria-label="Video dialog">
		<div class="lightbox-backdrop" data-close></div>
		<div class="lightbox-content" role="document">
			<button class="lightbox-close" aria-label="Close video" data-close>&times;</button>
			<div class="lightbox-body" id="lightbox-body"></div>
		</div>
	</div>

	<script src="assets/js/main.js"></script>
</body>
</html> 